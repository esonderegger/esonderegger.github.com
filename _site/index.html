<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8" />
<title>Evan Sonderegger on github</title>
<link href="/css/basic.css" rel="stylesheet" />
<link href='http://fonts.googleapis.com/css?family=PT+Sans:700' rel='stylesheet' type='text/css'>
<link href='http://fonts.googleapis.com/css?family=PT+Serif' rel='stylesheet' type='text/css'>
</head>
<body>
<header>
<nav class="container">
<span id="dummy" class="grid4 first"></span>
<ul>
<li class="grid2 first"><a href="/" title="Home">Home</a></li>
<li class="grid2"><a href="/resume" title="Resume">Résumé</a></li>
<li class="grid2"><a href="/projects" title="Projects">Projects</a></li>
<li class="grid2"><a href="/photography" title="Photography">Photography</a></li>
</ul>
<span id="dummy" class="grid4"></span>
</nav>
</header>
<div id="content" class="container">

<h4><a href="/2012/02/23/mastered-for-itunes.html">Mastered for iTunes</a></h4>
<p>When I saw this headline on HN I thought to myself &#8220;Wow, Apple is finally implementing ReplayGain in iTunes and on iDevices?&#8221;. Sadly this is not the case. It could do more for the quality of music on iTunes than any increases in bit depth or sample rate.</p>

<p>A quick word on bit depth: 16 bit audio gives us a potential signal to noise ratio of 96dB, which is plenty. The reason why we record in 24 bits is for increased headroom. If I make a 24 bit recording (potential S/N ratio of 144dB, but 124dB with current converters), but my loudest peak is at -18dB, I still have a S/N ratio of 106dB. I can then bring it into a DAW, give it 18dB of digital gain, and my S/N ratio of the 16 bit output file will still be 96dB.</p>

<p>Having a higher fidelity acquisition format than delivery format is not unique to audio. This is why photographers may shoot in RAW, but output to jpeg or tiff and why HD video is edited in 145Mbit/s, but delivered on blu-ray in 40 Mbit/s. It allows for some tweaking in post-production that doesn&#8217;t come at the expense of not maximizing the potential of the delivery format.</p>

<p>As for bit rates, I think most of the negative perceptions about digital audio come from back when iTunes&#8217;s default encoding was 128 kbps and ADC technology was still maturing as well as a knee-jerk reaction to lossy compression in general. When I make classical recordings available for web release I use LAME at the V2 setting. Obviously, what bit rate is &#8220;good enough&#8221; is dependent on program material, but for me that&#8217;s a reasonably small file size where I don&#8217;t hear compression artifacts. I know they take some crap for it, but I think Apple choosing 256kbps VBR AAC to be their iTunes plus setting was a good choice.</p>

<p>I don&#8217;t attempt to fully understand the business implications of these decisions, but &#8220;mastered for iTunes&#8221; appears to be more gimmick than substance. It may be that Apple is holding on to the high resolution master files for future ALAC release. The engineers quoted in the article talking about having to compensate for AAC&#8217;s losses are almost certainly talking about 128kbps. A great-sounding recording mastered at 16bit/44.1khz, will still sound great when properly encoded.</p>

<p>Also, Apples &#8220;Mastered for iTunes&#8221; technology brief seems to be written with hobbyist engineers in mind. I can&#8217;t imagine any competent mastering engineer finding it useful. Just further evidence that audio mastering as a craft is on its way out.</p>


</div>
</body>
</html>